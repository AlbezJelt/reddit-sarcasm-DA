{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "with open('data/X_train_new.txt', 'r') as f:\n",
                "    X = list(map(str, f.read().splitlines()))\n",
                "with open('data/Y_train_new.txt', 'r') as f:\n",
                "    Y = list(map(int, f.read().splitlines()))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "x_train, x_val, y_train, y_val = train_test_split(X, Y,\n",
                "    test_size=0.33,\n",
                "    random_state=996,\n",
                "    stratify=Y\n",
                "    )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import os\n",
                "import numpy as np\n",
                "\n",
                "path_to_glove_file = \"model/w2v_new_200.txt\"\n",
                "\n",
                "embeddings_index = {}\n",
                "with open(path_to_glove_file) as f:\n",
                "    for line in f:\n",
                "        word, coefs = line.split(maxsplit=1)\n",
                "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
                "        embeddings_index[word] = coefs\n",
                "\n",
                "print(\"Found %s word vectors.\" % len(embeddings_index))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import json\n",
                "from collections import Counter\n",
                "from itertools import chain\n",
                "\n",
                "# Reducing vocabulary size by cutting some low frequency words\n",
                "VOCAB = [word for word, freq in Counter(chain.from_iterable(sentence.split(' ') for sentence in X)).items() if freq >= 10 and word != '']\n",
                "\n",
                "# Load pre constructed vocabulary\n",
                "# with open('data/dictionary.json', 'r') as f:\n",
                "#     VOCAB = list(json.load(f).keys())[:len(embeddings_index)-1]\n",
                "\n",
                "VOCAB_SIZE = len(VOCAB)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
                "import tensorflow as tf\n",
                "\n",
                "# Max token is VOCAB_SIZE + 2 because keras reserve 2 more token, one for unseen token, one for whitespace\n",
                "vectorizer = TextVectorization(max_tokens=VOCAB_SIZE + 2, output_sequence_length=300, vocabulary=VOCAB, ngrams=3)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "voc = vectorizer.get_vocabulary()\n",
                "word_index = dict(zip(voc, range(len(voc))))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "num_tokens = len(voc) + 2\n",
                "embedding_dim = 200\n",
                "hits = 0\n",
                "misses = 0\n",
                "\n",
                "# Prepare embedding matrix\n",
                "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
                "for word, i in word_index.items():\n",
                "    embedding_vector = embeddings_index.get(word)\n",
                "    if embedding_vector is not None:\n",
                "        # Words not found in embedding index will be all-zeros.\n",
                "        # This includes the representation for \"padding\" and \"OOV\"\n",
                "        embedding_matrix[i] = embedding_vector\n",
                "        hits += 1\n",
                "    else:\n",
                "        misses += 1\n",
                "print(\"Converted %d words (%d misses)\" % (hits, misses))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from tensorflow.keras.layers import Embedding\n",
                "from tensorflow.keras import initializers\n",
                "\n",
                "# embedding_layer = Embedding(\n",
                "#     num_tokens,\n",
                "#     embedding_dim,\n",
                "#     embeddings_initializer=initializers.Constant(embedding_matrix),\n",
                "#     trainable=False,\n",
                "# )\n",
                "\n",
                "embedding_layer = Embedding(\n",
                "    num_tokens,\n",
                "    embedding_dim\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from tensorflow.keras import layers\n",
                "from tensorflow import keras\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.regularizers import l1_l2\t\n",
                "\n",
                "int_sequences_input = keras.Input(shape=(1,), dtype=tf.string)\n",
                "vect = vectorizer(int_sequences_input)\n",
                "embedded_sequences = embedding_layer(vect)\n",
                "\n",
                "x = layers.Conv1D(32, 3, strides=2, padding=\"same\")(embedded_sequences)\n",
                "x = layers.BatchNormalization()(x)\n",
                "x = layers.Activation(\"relu\")(x)\n",
                "x = layers.Conv1D(64, 3, padding=\"same\")(x)\n",
                "x = layers.BatchNormalization()(x)\n",
                "x = layers.Activation(\"relu\")(x)\n",
                "\n",
                "previous_block_activation = x\n",
                "\n",
                "for size in [128, 256, 512, 728]:\n",
                "    x = layers.Activation(\"relu\")(x)\n",
                "    x = layers.Conv1D(size, 3, padding=\"same\")(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation(\"relu\")(x)\n",
                "    x = layers.Conv1D(size, 3, padding=\"same\")(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "\n",
                "    x = layers.MaxPooling1D(3, strides=2, padding=\"same\")(x)\n",
                "\n",
                "    residual = layers.Conv1D(size, 1, strides=2, padding=\"same\")(\n",
                "        previous_block_activation\n",
                "    )\n",
                "\n",
                "    x = layers.Add()([x, residual])\n",
                "    previous_block_activation = x\n",
                "\n",
                "x = layers.SeparableConv1D(1024, 3, padding=\"same\")(x)\n",
                "x = layers.BatchNormalization()(x)\n",
                "x = layers.Activation(\"relu\")(x)\n",
                "x = layers.Dropout(0.5)(x)\n",
                "\n",
                "x = layers.LSTM(256, dropout=0.3, return_sequences=True)(x)\n",
                "x = layers.LSTM(256, dropout=0.3)(x)\n",
                "x = layers.Dense(200)(x)\n",
                "x = layers.BatchNormalization()(x)\n",
                "x = layers.Activation(\"relu\")(x)\n",
                "x = layers.Dense(100)(x)\n",
                "x = layers.BatchNormalization()(x)\n",
                "x = layers.Activation(\"relu\")(x)\n",
                "preds = layers.Dense(1, activation=\"linear\")(x)\n",
                "model = keras.Model(int_sequences_input, preds)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "model.summary()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.losses import BinaryCrossentropy\n",
                "from tensorflow.keras.metrics import Precision, Recall, AUC, BinaryAccuracy\n",
                "\n",
                "model.compile(\n",
                "    loss=BinaryCrossentropy(from_logits=True), optimizer=Adam(learning_rate=0.001), metrics=[BinaryAccuracy()]\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
                "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)\n",
                "early_stop = EarlyStopping(monitor='val_loss', patience=8)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "model.fit(x_train, y_train, batch_size=128, epochs=100, validation_data=(x_val, y_val), callbacks=[reduce_lr, early_stop])"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}